Only in ../s5d: babel.html
Only in s5d_limited: canto.conf
Only in s5d_limited: canto_limited.conf
Only in s5d_limited: decode_lr0.25_old.txt
Only in s5d_limited: decode_lr1.0.txt
Only in s5d_limited: decode_lr1.txt
Only in ../s5d: EXAMPLE.vietnamese
Only in s5d_limited: .gitignore
Only in s5d_limited/local/chain: run_ivector_common_aishell2_bab_aextractor.sh
diff '--exclude=out*.txt' '--exclude=exp' '--exclude=data' '--exclude=mfcc*' -ru ../s5d/local/chain/run_ivector_common_aishell2_bab.sh s5d_limited/local/chain/run_ivector_common_aishell2_bab.sh
--- ../s5d/local/chain/run_ivector_common_aishell2_bab.sh	2018-12-20 18:11:17.643868793 +0000
+++ s5d_limited/local/chain/run_ivector_common_aishell2_bab.sh	2018-12-20 17:30:22.936752809 +0000
@@ -9,12 +9,17 @@
 
 # copied from libri-speech
 
+# Bryan Li (bl2557), Xinyue Wang (xw2368)
+# This script extracts the i-vectors, and does any necessary data processing for that as well.
+# It is usually called by a chain script (in local/chain/tuning).
+# See comments below for more detail.
+
 stage=0
 nj=30
 train_set=train   # you might set this to e.g. train.
 gmm=tri4          # This specifies a GMM-dir from the features
-                          # of the type you're training the system on;
-                          # it should contain alignments for 'train_set'.
+                  # of the type you're training the system on;
+                  # it should contain alignments for 'train_set'.
 langdir=data/langp/tri4_ali
 
 num_threads_ubm=12
diff '--exclude=out*.txt' '--exclude=exp' '--exclude=data' '--exclude=mfcc*' -ru ../s5d/local/chain/run_ivector_common.sh s5d_limited/local/chain/run_ivector_common.sh
--- ../s5d/local/chain/run_ivector_common.sh	2018-12-20 18:11:17.583868815 +0000
+++ s5d_limited/local/chain/run_ivector_common.sh	2018-12-15 18:18:27.925653281 +0000
@@ -12,13 +12,13 @@
 stage=0
 nj=30
 train_set=train   # you might set this to e.g. train.
-gmm=tri5          # This specifies a GMM-dir from the features
+gmm=tri4          # This specifies a GMM-dir from the features
                           # of the type you're training the system on;
                           # it should contain alignments for 'train_set'.
-langdir=data/langp/tri5_ali
+langdir=data/langp/tri4_ali
 
 num_threads_ubm=12
-nnet3_affix=
+nnet3_affix=_cleaned
 
 . ./cmd.sh
 . ./path.sh
diff '--exclude=out*.txt' '--exclude=exp' '--exclude=data' '--exclude=mfcc*' -ru ../s5d/local/chain/run_tdnn_aishell2_bab.sh s5d_limited/local/chain/run_tdnn_aishell2_bab.sh
--- ../s5d/local/chain/run_tdnn_aishell2_bab.sh	2018-12-20 18:11:17.699868773 +0000
+++ s5d_limited/local/chain/run_tdnn_aishell2_bab.sh	2018-12-20 17:39:42.280551916 +0000
@@ -12,6 +12,10 @@
 # Training: The transferred layers are retrained with smaller learning-rate,
 # while new added layers are trained with larger learning rate using babel (cantonese) data.
 
+# Bryan Li (bl2557), Xinyue Wang (xw2368)
+# This script runs the chain model.
+# See comments below for more detail.
+
 set -e
 
 # configs for 'chain'
@@ -19,24 +23,26 @@
 stage=0
 train_stage=-10
 get_egs_stage=-10
-dir=exp/chain/tdnn_aishell2_bab_1a
+nnet3_affix=
+train_set=train${nnet3_affix}
+gmm=tri4${nnet3_affix}
+dir=exp/chain/tdnn_aishell2_bab_1a_lr1.0 # remember to change for different runs
 xent_regularize=0.1
 chunk_width=150,110,90
 
-
-gmm_dir=exp/tri4
-ali_dir=exp/tri4_ali_train_sp
-tree_dir=exp/chain/tree_1a
-lang=data/lang_chain_1a
-train_data_dir=data/train_sp_hires
-train_data_dir_lores=data/train_sp
-lat_dir=exp/chain/tri4_train_sp_lats
-lang_dir=data/langp/tri4
+gmm_dir=exp/$gmm
+ali_dir=exp/${gmm}_ali_${train_set}_sp
+tree_dir=exp/chain${nnet3_affix}/tree_1a
+lat_dir=exp/chain${nnet3_affix}/tri4${nnet3_affix}_train_sp_lats
+lang=data/lang_chain${nnet3_affix}_1a
+train_data_dir=data/${train_set}_sp_hires
+train_data_dir_lores=data/${train_set}_sp
+lang_dir=data/langp/tri4 # instead of tri5_ali
 
 # configs for transfer learning
 src_mdl=../../aishell2/s5/exp/chain/tdnn_1b_all_sp/final.mdl # input chain model
 							     # trained on aishell2
-							     # This model is transfered to th etarget domain
+							     # This model is transfered to the target domain
 src_mfcc_config=../../aishell2/s5/conf/mfcc_hires.conf # mfcc config used to extract higher dim
                                                        # mfcc features for ivector and DNN training
                                                        # in the source domain.
@@ -44,11 +50,12 @@
                                                                   # source data. The ivector for target data is extracted using this extractor.
                          					  # It should be nonempty, if ivector is used in the source model training.
 common_egs_dir=
+#common_egs_dir=exp/chain/tdnn_aishell2_bab_1a_lr0.25/egs # uncomment to use old examples, instead of regenerating new ones
 primary_lr_factor=0.25 # The learning-rate factor for transferred layers from source
                        # model. e.g. if 0, the paramters transferred from source model
                        # are fixed.
                        # The learning-rate factor for new added layers is 1.0.
-# nnet_affix=_online_aishell2
+
 # End configuration section
 
 echo "$0 $@"  # Print the command line for logging
@@ -65,12 +72,12 @@
 EOF
 fi
 
-local/chain/run_ivector_common_aishell2_bab.sh --stage $stage
+local/chain/run_ivector_common_aishell2_bab.sh --stage $stage \
+                                  --nnet3-affix "$nnet3_affix" \
+                                  --train-set $train_set \
+                                  --gmm $gmm
                                   # --nj $nj \
-                                  # --train-set $train_set \
-                                  # --gmm $gmm \
                                   # --num-threads-ubm $num_threads_ubm \
-                                  # --nnet3-affix "$nnet3_affix"
 
 required_files="$src_mfcc_config $src_mdl"
 use_ivector=false
@@ -127,7 +134,6 @@
   else
     rm -r $lang 2>/dev/null || true
     cp -r $lang_dir $lang
-    # cp -r data/langp/tri4_ali $lang
     silphonelist=$(cat $lang/phones/silence.csl) || exit 1;
     nonsilphonelist=$(cat $lang/phones/nonsilence.csl) || exit 1;
     # Use our special topology... note that later on may have to tune this
@@ -147,16 +153,21 @@
     --cmd "$train_cmd" 5000 $train_data_dir_lores $lang $ali_dir $tree_dir || exit 1;
 fi
 
+# this section contains the config file for transfer learning. tdnn1 - tdnn9 are transferred and learned at the set lr.
+# tdnn10 to output layers are initialized to 0 and learned at lr=1.0.
 if [ $stage -le 17 ]; then
   echo "$0: Create neural net configs using the xconfig parser for";
-  echo " generating new layers, that are specific to rm. These layers ";
-  echo " are added to the transferred part of the wsj network.";
+  echo " generating new layers, that are specific to babel. These layers ";
+  echo " are added to the transferred part of the aishell2 network.";
   num_targets=$(tree-info --print-args=false $tree_dir/tree |grep num-pdfs|awk '{print $2}')
   learning_rate_factor=$(echo "print 0.5/$xent_regularize" | python)
+  opts="l2-regularize=0.002"
+  linear_opts="orthonormal-constraint=1.0"
+  output_opts="l2-regularize=0.0005 bottleneck-dim=256"
   mkdir -p $dir
   mkdir -p $dir/configs
   cat <<EOF > $dir/configs/network.xconfig
-  relu-batchnorm-dropout-layer name=tdnn9a input=Append(tdnn9.batchnorm@-3,tdnn9.batchnorm) dim=1280
+  relu-batchnorm-dropout-layer name=tdnn9a input=Append(tdnn9.dropout,tdnn9.dropout@3, tdnn8l, tdnn6l, tdnn4l) dim=1280
   linear-component name=tdnn10l dim=256 $linear_opts input=Append(-3,0)
   relu-batchnorm-dropout-layer name=tdnn10 $opts input=Append(0,3) dim=1280
   linear-component name=tdnn11l dim=256 $linear_opts input=Append(-3,0)
@@ -184,14 +195,11 @@
 fi
 
 if [ $stage -le 18 ]; then
-  echo "$0: generate egs for chain to train new model on rm dataset."
-  if [[ $(hostname -f) == *.clsp.jhu.edu ]] && [ ! -d $dir/egs/storage ]; then
-    utils/create_split_dir.pl \
-     /export/b0{3,4,5,6}/$USER/kaldi-data/egs/rm-$(date +'%m_%d_%H_%M')/s5/$dir/egs/storage $dir/egs/storage
-  fi
+  
+  echo "$0: generate egs for chain to train new model on babel dataset."
+
   ivector_dir=
-  # if $use_ivector; then ivector_dir="exp/nnet2${nnet_affix}/ivectors" ; fi
-  if $use_ivector; then ivector_dir="exp/nnet3/ivectors_train_sp_hires/" ; fi
+  if $use_ivector; then ivector_dir="exp/nnet3${nnet_affix}/ivectors_${train_set}_sp_hires" ; fi
 
   steps/nnet3/chain/train.py --stage $train_stage \
     --cmd "$decode_cmd" \
@@ -226,15 +234,11 @@
   # Note: it might appear that this $lang directory is mismatched, and it is as
   # far as the 'topo' is concerned, but this script doesn't read the 'topo' from
   # the lang directory.
-  ivec_opt=""
-  if $use_ivector;then
-    ivec_opt="--online-ivector-dir exp/nnet2${nnet_affix}/ivectors_test"
-  fi
-  utils/mkgraph.sh --self-loop-scale 1.0 data/lang $dir $dir/graph
-  steps/nnet3/decode.sh --acwt 1.0 --post-decode-acwt 10.0 \
-    --scoring-opts "--min-lmwt 1" \
-    --nj 20 --cmd "$decode_cmd" $ivec_opt \
-    $dir/graph data/test_hires $dir/decode || exit 1;
+  # ivec_opt=""
+  # if $use_ivector;then
+    # ivec_opt="--online-ivector-dir exp/nnet2${nnet_affix}/ivectors_test"
+  # fi
+  utils/mkgraph.sh --self-loop-scale 1.0 data/lang/ $dir $dir/graph
 fi
 wait;
 exit 0;
diff '--exclude=out*.txt' '--exclude=exp' '--exclude=data' '--exclude=mfcc*' -ru ../s5d/local/chain/run_tdnn.sh s5d_limited/local/chain/run_tdnn.sh
--- ../s5d/local/chain/run_tdnn.sh	2018-12-20 18:11:17.695868775 +0000
+++ s5d_limited/local/chain/run_tdnn.sh	2018-12-20 17:48:04.492370333 +0000
@@ -16,6 +16,9 @@
 #                  xent:train/valid[55,84,final]=(-2.44,-2.32,-2.32/-2.63,-2.57,-2.56)
 #                  logprob:train/valid[55,84,final]=(-0.214,-0.192,-0.191/-0.281,-0.276,-0.275)
 
+# Bryan Li (bl2557), Xinyue Wang (xw2368)
+# This script runs the chain model. It is the same as the original BABEL script with slight modifications for our directory structure.
+# See comments below for more detail.
 
 set -e -o pipefail
 
@@ -24,7 +27,7 @@
 stage=0
 nj=30
 train_set=train_cleaned
-gmm=tri4_cleaned  # the gmm for the target data
+gmm=tri4_cleaned  # the gmm for the target data # only use up to tri4, as tri5_ali is not generated for our GMM model
 langdir=data/langp/tri4
 num_threads_ubm=12
 nnet3_affix=_cleaned  # cleanup affix for nnet3 and chain dirs, e.g. _cleaned
@@ -120,6 +123,8 @@
       --cmd "$train_cmd" 4000 ${lores_train_data_dir} data/lang_chain $ali_dir $tree_dir
 fi
 
+# same recipe as before. We tried setting this to the aishell2 recipe (with no transfer learning involved),
+# but found the chain model could not be decoded. The reason for this is discussed in our paper.
 xent_regularize=0.1
 if [ $stage -le 17 ]; then
   mkdir -p $dir
@@ -211,7 +216,7 @@
   # Note: it might appear that this data/lang_chain directory is mismatched, and it is as
   # far as the 'topo' is concerned, but this script doesn't read the 'topo' from
   # the lang directory.
-  utils/mkgraph.sh --self-loop-scale 1.0 data/langp_test $dir $dir/graph
+  utils/mkgraph.sh --self-loop-scale 1.0 data/lang $dir $dir/graph
 fi
 
 exit 0
Only in s5d_limited/local/chain/tuning: run_tdnn_aishell2_bab_1a_aivector.sh
diff '--exclude=out*.txt' '--exclude=exp' '--exclude=data' '--exclude=mfcc*' -ru ../s5d/local/chain/tuning/run_tdnn_aishell2_bab_1a.sh s5d_limited/local/chain/tuning/run_tdnn_aishell2_bab_1a.sh
--- ../s5d/local/chain/tuning/run_tdnn_aishell2_bab_1a.sh	2018-12-20 18:11:17.699868773 +0000
+++ s5d_limited/local/chain/tuning/run_tdnn_aishell2_bab_1a.sh	2018-12-20 17:39:42.280551916 +0000
@@ -12,6 +12,10 @@
 # Training: The transferred layers are retrained with smaller learning-rate,
 # while new added layers are trained with larger learning rate using babel (cantonese) data.
 
+# Bryan Li (bl2557), Xinyue Wang (xw2368)
+# This script runs the chain model.
+# See comments below for more detail.
+
 set -e
 
 # configs for 'chain'
@@ -19,24 +23,26 @@
 stage=0
 train_stage=-10
 get_egs_stage=-10
-dir=exp/chain/tdnn_aishell2_bab_1a
+nnet3_affix=
+train_set=train${nnet3_affix}
+gmm=tri4${nnet3_affix}
+dir=exp/chain/tdnn_aishell2_bab_1a_lr1.0 # remember to change for different runs
 xent_regularize=0.1
 chunk_width=150,110,90
 
-
-gmm_dir=exp/tri4
-ali_dir=exp/tri4_ali_train_sp
-tree_dir=exp/chain/tree_1a
-lang=data/lang_chain_1a
-train_data_dir=data/train_sp_hires
-train_data_dir_lores=data/train_sp
-lat_dir=exp/chain/tri4_train_sp_lats
-lang_dir=data/langp/tri4
+gmm_dir=exp/$gmm
+ali_dir=exp/${gmm}_ali_${train_set}_sp
+tree_dir=exp/chain${nnet3_affix}/tree_1a
+lat_dir=exp/chain${nnet3_affix}/tri4${nnet3_affix}_train_sp_lats
+lang=data/lang_chain${nnet3_affix}_1a
+train_data_dir=data/${train_set}_sp_hires
+train_data_dir_lores=data/${train_set}_sp
+lang_dir=data/langp/tri4 # instead of tri5_ali
 
 # configs for transfer learning
 src_mdl=../../aishell2/s5/exp/chain/tdnn_1b_all_sp/final.mdl # input chain model
 							     # trained on aishell2
-							     # This model is transfered to th etarget domain
+							     # This model is transfered to the target domain
 src_mfcc_config=../../aishell2/s5/conf/mfcc_hires.conf # mfcc config used to extract higher dim
                                                        # mfcc features for ivector and DNN training
                                                        # in the source domain.
@@ -44,11 +50,12 @@
                                                                   # source data. The ivector for target data is extracted using this extractor.
                          					  # It should be nonempty, if ivector is used in the source model training.
 common_egs_dir=
+#common_egs_dir=exp/chain/tdnn_aishell2_bab_1a_lr0.25/egs # uncomment to use old examples, instead of regenerating new ones
 primary_lr_factor=0.25 # The learning-rate factor for transferred layers from source
                        # model. e.g. if 0, the paramters transferred from source model
                        # are fixed.
                        # The learning-rate factor for new added layers is 1.0.
-# nnet_affix=_online_aishell2
+
 # End configuration section
 
 echo "$0 $@"  # Print the command line for logging
@@ -65,12 +72,12 @@
 EOF
 fi
 
-local/chain/run_ivector_common_aishell2_bab.sh --stage $stage
+local/chain/run_ivector_common_aishell2_bab.sh --stage $stage \
+                                  --nnet3-affix "$nnet3_affix" \
+                                  --train-set $train_set \
+                                  --gmm $gmm
                                   # --nj $nj \
-                                  # --train-set $train_set \
-                                  # --gmm $gmm \
                                   # --num-threads-ubm $num_threads_ubm \
-                                  # --nnet3-affix "$nnet3_affix"
 
 required_files="$src_mfcc_config $src_mdl"
 use_ivector=false
@@ -127,7 +134,6 @@
   else
     rm -r $lang 2>/dev/null || true
     cp -r $lang_dir $lang
-    # cp -r data/langp/tri4_ali $lang
     silphonelist=$(cat $lang/phones/silence.csl) || exit 1;
     nonsilphonelist=$(cat $lang/phones/nonsilence.csl) || exit 1;
     # Use our special topology... note that later on may have to tune this
@@ -147,16 +153,21 @@
     --cmd "$train_cmd" 5000 $train_data_dir_lores $lang $ali_dir $tree_dir || exit 1;
 fi
 
+# this section contains the config file for transfer learning. tdnn1 - tdnn9 are transferred and learned at the set lr.
+# tdnn10 to output layers are initialized to 0 and learned at lr=1.0.
 if [ $stage -le 17 ]; then
   echo "$0: Create neural net configs using the xconfig parser for";
-  echo " generating new layers, that are specific to rm. These layers ";
-  echo " are added to the transferred part of the wsj network.";
+  echo " generating new layers, that are specific to babel. These layers ";
+  echo " are added to the transferred part of the aishell2 network.";
   num_targets=$(tree-info --print-args=false $tree_dir/tree |grep num-pdfs|awk '{print $2}')
   learning_rate_factor=$(echo "print 0.5/$xent_regularize" | python)
+  opts="l2-regularize=0.002"
+  linear_opts="orthonormal-constraint=1.0"
+  output_opts="l2-regularize=0.0005 bottleneck-dim=256"
   mkdir -p $dir
   mkdir -p $dir/configs
   cat <<EOF > $dir/configs/network.xconfig
-  relu-batchnorm-dropout-layer name=tdnn9a input=Append(tdnn9.batchnorm@-3,tdnn9.batchnorm) dim=1280
+  relu-batchnorm-dropout-layer name=tdnn9a input=Append(tdnn9.dropout,tdnn9.dropout@3, tdnn8l, tdnn6l, tdnn4l) dim=1280
   linear-component name=tdnn10l dim=256 $linear_opts input=Append(-3,0)
   relu-batchnorm-dropout-layer name=tdnn10 $opts input=Append(0,3) dim=1280
   linear-component name=tdnn11l dim=256 $linear_opts input=Append(-3,0)
@@ -184,14 +195,11 @@
 fi
 
 if [ $stage -le 18 ]; then
-  echo "$0: generate egs for chain to train new model on rm dataset."
-  if [[ $(hostname -f) == *.clsp.jhu.edu ]] && [ ! -d $dir/egs/storage ]; then
-    utils/create_split_dir.pl \
-     /export/b0{3,4,5,6}/$USER/kaldi-data/egs/rm-$(date +'%m_%d_%H_%M')/s5/$dir/egs/storage $dir/egs/storage
-  fi
+  
+  echo "$0: generate egs for chain to train new model on babel dataset."
+
   ivector_dir=
-  # if $use_ivector; then ivector_dir="exp/nnet2${nnet_affix}/ivectors" ; fi
-  if $use_ivector; then ivector_dir="exp/nnet3/ivectors_train_sp_hires/" ; fi
+  if $use_ivector; then ivector_dir="exp/nnet3${nnet_affix}/ivectors_${train_set}_sp_hires" ; fi
 
   steps/nnet3/chain/train.py --stage $train_stage \
     --cmd "$decode_cmd" \
@@ -226,15 +234,11 @@
   # Note: it might appear that this $lang directory is mismatched, and it is as
   # far as the 'topo' is concerned, but this script doesn't read the 'topo' from
   # the lang directory.
-  ivec_opt=""
-  if $use_ivector;then
-    ivec_opt="--online-ivector-dir exp/nnet2${nnet_affix}/ivectors_test"
-  fi
-  utils/mkgraph.sh --self-loop-scale 1.0 data/lang $dir $dir/graph
-  steps/nnet3/decode.sh --acwt 1.0 --post-decode-acwt 10.0 \
-    --scoring-opts "--min-lmwt 1" \
-    --nj 20 --cmd "$decode_cmd" $ivec_opt \
-    $dir/graph data/test_hires $dir/decode || exit 1;
+  # ivec_opt=""
+  # if $use_ivector;then
+    # ivec_opt="--online-ivector-dir exp/nnet2${nnet_affix}/ivectors_test"
+  # fi
+  utils/mkgraph.sh --self-loop-scale 1.0 data/lang/ $dir $dir/graph
 fi
 wait;
 exit 0;
Only in ../s5d/local/chain/tuning: run_tdnn_aishell2_bab_1b.sh
Only in ../s5d/local/chain/tuning: run_tdnn_aishell2_bab_1c.sh
diff '--exclude=out*.txt' '--exclude=exp' '--exclude=data' '--exclude=mfcc*' -ru ../s5d/local/chain/tuning/run_tdnn.sh s5d_limited/local/chain/tuning/run_tdnn.sh
--- ../s5d/local/chain/tuning/run_tdnn.sh	2018-12-20 18:11:17.695868775 +0000
+++ s5d_limited/local/chain/tuning/run_tdnn.sh	2018-12-20 17:48:04.492370333 +0000
@@ -16,6 +16,9 @@
 #                  xent:train/valid[55,84,final]=(-2.44,-2.32,-2.32/-2.63,-2.57,-2.56)
 #                  logprob:train/valid[55,84,final]=(-0.214,-0.192,-0.191/-0.281,-0.276,-0.275)
 
+# Bryan Li (bl2557), Xinyue Wang (xw2368)
+# This script runs the chain model. It is the same as the original BABEL script with slight modifications for our directory structure.
+# See comments below for more detail.
 
 set -e -o pipefail
 
@@ -24,7 +27,7 @@
 stage=0
 nj=30
 train_set=train_cleaned
-gmm=tri4_cleaned  # the gmm for the target data
+gmm=tri4_cleaned  # the gmm for the target data # only use up to tri4, as tri5_ali is not generated for our GMM model
 langdir=data/langp/tri4
 num_threads_ubm=12
 nnet3_affix=_cleaned  # cleanup affix for nnet3 and chain dirs, e.g. _cleaned
@@ -120,6 +123,8 @@
       --cmd "$train_cmd" 4000 ${lores_train_data_dir} data/lang_chain $ali_dir $tree_dir
 fi
 
+# same recipe as before. We tried setting this to the aishell2 recipe (with no transfer learning involved),
+# but found the chain model could not be decoded. The reason for this is discussed in our paper.
 xent_regularize=0.1
 if [ $stage -le 17 ]; then
   mkdir -p $dir
@@ -211,7 +216,7 @@
   # Note: it might appear that this data/lang_chain directory is mismatched, and it is as
   # far as the 'topo' is concerned, but this script doesn't read the 'topo' from
   # the lang directory.
-  utils/mkgraph.sh --self-loop-scale 1.0 data/langp_test $dir $dir/graph
+  utils/mkgraph.sh --self-loop-scale 1.0 data/lang $dir $dir/graph
 fi
 
 exit 0
Only in s5d_limited/local/nist_eval: create_no_indus.sh
diff '--exclude=out*.txt' '--exclude=exp' '--exclude=data' '--exclude=mfcc*' -ru ../s5d/local/nnet3/run_ivector_common.sh s5d_limited/local/nnet3/run_ivector_common.sh
--- ../s5d/local/nnet3/run_ivector_common.sh	2018-12-20 18:11:17.583868815 +0000
+++ s5d_limited/local/nnet3/run_ivector_common.sh	2018-12-15 18:18:27.925653281 +0000
@@ -12,13 +12,13 @@
 stage=0
 nj=30
 train_set=train   # you might set this to e.g. train.
-gmm=tri5          # This specifies a GMM-dir from the features
+gmm=tri4          # This specifies a GMM-dir from the features
                           # of the type you're training the system on;
                           # it should contain alignments for 'train_set'.
-langdir=data/langp/tri5_ali
+langdir=data/langp/tri4_ali
 
 num_threads_ubm=12
-nnet3_affix=
+nnet3_affix=_cleaned
 
 . ./cmd.sh
 . ./path.sh
Only in s5d_limited/local: prepare_aishell2_bab_lang.sh
Only in s5d_limited/local: run-1-main-transfer-learn.sh
diff '--exclude=out*.txt' '--exclude=exp' '--exclude=data' '--exclude=mfcc*' -ru ../s5d/local/run_kws_stt_task.sh s5d_limited/local/run_kws_stt_task.sh
--- ../s5d/local/run_kws_stt_task.sh	2018-12-20 18:11:17.783868743 +0000
+++ s5d_limited/local/run_kws_stt_task.sh	2018-12-16 02:34:42.733844710 +0000
@@ -55,13 +55,14 @@
 ##NB: The first ".done" files are used for backward compatibility only
 ##NB: should be removed in a near future...
 model=`dirname $decode_dir`/${iter}.mdl
+echo 'hellloooo'
 if ! $skip_stt ; then
   if  [ ! -f $decode_dir/.score.done ] && [ ! -f $decode_dir/.done.score ]; then
-    local/lattice_to_ctm.sh --cmd "$cmd" --word-ins-penalty $wip \
-      --min-lmwt ${min_lmwt} --max-lmwt ${max_lmwt} \
-      --model $model \
-      $data_dir $lang_dir $decode_dir
-
+    # local/lattice_to_ctm.sh --cmd "$cmd" --word-ins-penalty $wip \
+      # --min-lmwt ${min_lmwt} --max-lmwt ${max_lmwt} \
+      # --model $model \
+      # $data_dir $lang_dir $decode_dir
+    
     if ! $skip_scoring ; then
       local/score_stm.sh --cmd "$cmd"  --cer $cer \
         --min-lmwt ${min_lmwt} --max-lmwt ${max_lmwt}\
diff '--exclude=out*.txt' '--exclude=exp' '--exclude=data' '--exclude=mfcc*' -ru ../s5d/local/score.sh s5d_limited/local/score.sh
--- ../s5d/local/score.sh	2018-12-20 18:11:17.783868743 +0000
+++ s5d_limited/local/score.sh	2018-12-16 02:34:42.733844710 +0000
@@ -55,13 +55,14 @@
 ##NB: The first ".done" files are used for backward compatibility only
 ##NB: should be removed in a near future...
 model=`dirname $decode_dir`/${iter}.mdl
+echo 'hellloooo'
 if ! $skip_stt ; then
   if  [ ! -f $decode_dir/.score.done ] && [ ! -f $decode_dir/.done.score ]; then
-    local/lattice_to_ctm.sh --cmd "$cmd" --word-ins-penalty $wip \
-      --min-lmwt ${min_lmwt} --max-lmwt ${max_lmwt} \
-      --model $model \
-      $data_dir $lang_dir $decode_dir
-
+    # local/lattice_to_ctm.sh --cmd "$cmd" --word-ins-penalty $wip \
+      # --min-lmwt ${min_lmwt} --max-lmwt ${max_lmwt} \
+      # --model $model \
+      # $data_dir $lang_dir $decode_dir
+    
     if ! $skip_scoring ; then
       local/score_stm.sh --cmd "$cmd"  --cer $cer \
         --min-lmwt ${min_lmwt} --max-lmwt ${max_lmwt}\
diff '--exclude=out*.txt' '--exclude=exp' '--exclude=data' '--exclude=mfcc*' -ru ../s5d/path.sh s5d_limited/path.sh
--- ../s5d/path.sh	2018-11-17 15:44:31.740346587 +0000
+++ s5d_limited/path.sh	2018-12-15 05:29:31.816971586 +0000
@@ -10,3 +10,4 @@
 . $KALDI_ROOT/tools/config/common_path.sh
 
 export LC_ALL=C
+
Only in ../s5d: README.graphemic
Only in s5d_limited: README.md
Only in ../s5d: README.txt
Only in s5d_limited: report
Only in s5d_limited: report2.zip
Only in s5d_limited: report_final.zip
Only in ../s5d: results
Only in ../s5d: RESULTS.txt
diff '--exclude=out*.txt' '--exclude=exp' '--exclude=data' '--exclude=mfcc*' -ru ../s5d/run-1-main-aishell2_bab_limited.sh s5d_limited/run-1-main-aishell2_bab_limited.sh
--- ../s5d/run-1-main-aishell2_bab_limited.sh	2018-12-12 22:53:36.935072220 +0000
+++ s5d_limited/run-1-main-aishell2_bab_limited.sh	2018-12-20 17:09:10.057199726 +0000
@@ -2,6 +2,11 @@
 
 # This is not necessarily the top-level run.sh as it is in other directories.   see README.txt first.
 # modified to be similar to kaldi-trunk/egs/aishell2/s5/local/run_gmm.sh
+
+# Bryan Li (bl2557), Xinyue Wang (xw2368)
+# This script runs the GMM model. It is based on the original BABEL one, but extracts the same features as AISHELL2 does.
+# Runs up to LDA+MLLT triphone alignment. See comments below for more detail.
+
 tri5_only=false
 sgmm5_only=false
 denlats_only=false
@@ -103,7 +108,7 @@
 echo ---------------------------------------------------------------------
 echo "Starting mfcc feature extraction for data/train in mfcc on" `date`
 echo ---------------------------------------------------------------------
-
+# instead of PLP, extract MFCC
 if [ ! -f data/train/.mfcc.done ]; then
   if $use_pitch; then
     steps/make_mfcc_pitch.sh --pitch-config conf/pitch.conf --cmd "$train_cmd" --nj $train_nj data/train exp/make_mfcc/train mfcc
@@ -159,7 +164,8 @@
   steps/align_si.sh \
     --boost-silence $boost_sil --nj 12 --cmd "$train_cmd" \
     data/train_sub2 data/lang exp/mono exp/mono_ali_sub2
-
+    
+  # change number of leaves to match AISHELL2
   steps/train_deltas.sh \
     --boost-silence $boost_sil --cmd "$train_cmd" 4000 32000 \
     data/train_sub2 data/lang exp/mono_ali_sub2 exp/tri1
@@ -176,6 +182,7 @@
     --boost-silence $boost_sil --nj 24 --cmd "$train_cmd" \
     data/train_sub3 data/lang exp/tri1 exp/tri1_ali_sub3
 
+  # change number of leaves to match AISHELL2
   steps/train_deltas.sh \
     --boost-silence $boost_sil --cmd "$train_cmd" 7000 56000 \
     data/train_sub3 data/lang exp/tri1_ali_sub3 exp/tri2
@@ -194,7 +201,8 @@
   steps/align_si.sh \
     --boost-silence $boost_sil --nj $train_nj --cmd "$train_cmd" \
     data/train data/langp/tri2 exp/tri2 exp/tri2_ali
-
+  
+  # change number of leaves to match AISHELL2
   steps/train_deltas.sh \
     --boost-silence $boost_sil --cmd "$train_cmd" \
     7000 56000 data/train data/langp/tri2 exp/tri2_ali exp/tri3
@@ -214,6 +222,7 @@
     --boost-silence $boost_sil --nj $train_nj --cmd "$train_cmd" \
     data/train data/langp/tri3 exp/tri3 exp/tri3_ali
 
+  # change number of leaves to match AISHELL2
   steps/train_lda_mllt.sh \
     --boost-silence $boost_sil --cmd "$train_cmd" \
     10000 80000 data/train data/langp/tri3 exp/tri3_ali exp/tri4
Only in ../s5d: run-1-main-aishell2_bab.sh
Only in ../s5d: run-1-main-extend-lex.sh
Only in ../s5d: run-1-main.sh
Only in ../s5d: run-1-main-unicode-extend-lex.sh
Only in ../s5d: run-1-main-unicode.sh
diff '--exclude=out*.txt' '--exclude=exp' '--exclude=data' '--exclude=mfcc*' -ru ../s5d/run-4-anydecode-aishell2_bab.sh s5d_limited/run-4-anydecode-aishell2_bab.sh
--- ../s5d/run-4-anydecode-aishell2_bab.sh	2018-12-20 15:23:51.955398524 +0000
+++ s5d_limited/run-4-anydecode-aishell2_bab.sh	2018-12-20 17:13:39.941104731 +0000
@@ -1,4 +1,9 @@
 #!/bin/bash
+
+# Bryan Li (bl2557), Xinyue Wang (xw2368)
+# This script decodes a chain model.
+# Our contributions detailed in comments below.
+
 set -e
 set -o pipefail
 
@@ -6,18 +11,18 @@
 . ./lang.conf || exit 1;
 
 
-dir=dev2h.pem
+dir=dev2h.pem # change this to dev10h.pem for full dev set decoing
 kind=
 data_only=false
 fast_path=true
 skip_stt=false
-skip_kws=true
+skip_kws=true # not doing keyword search task
 skip_scoring=false
 cer=1
 tri5_only=false
 wip=0.5
 my_nj=30
-chain_model=chain/tdnn_aishell2_bab_1a_lr1.0
+chain_model=chain/tdnn_aishell2_bab_1a_lr1.0 # change this to the chain directory you wish to decode
 is_rnn=false
 extra_left_context=40
 extra_right_context=40
@@ -184,16 +189,6 @@
   touch ${dataset_dir}_hires/.done
 fi
 
-# if [ -f exp/nnet3/extractor/final.ie ] && \
-  # [ ! -f exp/nnet3/ivectors_$(basename $dataset_dir)/.done ] ;  then
-  # dataset=$(basename $dataset_dir)
-
-  # steps/online/nnet2/extract_ivectors_online.sh --cmd "$train_cmd" --nj $my_nj \
-    # ${dataset_dir}_hires exp/nnet3/extractor exp/nnet3/ivectors_$dataset || exit 1;
-
-  # touch exp/nnet3/ivectors_$dataset/.done
-# fi
-
 ####################################################################
 ##
 ## chain model decoding
@@ -234,6 +229,7 @@
     touch $decode/.done
   fi
   
+  # CER=1 to get CER
   local/run_kws_stt_task2.sh --cer $cer --max-states $max_states \
     --skip-scoring false --extra-kws false --wip $wip \
     --cmd "$decode_cmd" --skip-kws true --skip-stt $skip_stt  \
@@ -244,5 +240,11 @@
   echo "no chain model exp/$chain_model"
 fi
 
+# print best CER and WER to console
+echo "Best CER"
+grep Sum exp/${chain_model}/decode_dev2h.pem/score_*/*pem.char.ctm.sys | utils/best_wer.sh
+echo "Best WER"
+grep Sum exp/${chain_model}/decode_dev2h.pem/score_*/*pem.ctm.sys | utils/best_wer.sh
+
 echo "Everything looking good...."
 exit 0
diff '--exclude=out*.txt' '--exclude=exp' '--exclude=data' '--exclude=mfcc*' -ru ../s5d/run-4-anydecode.sh s5d_limited/run-4-anydecode.sh
--- ../s5d/run-4-anydecode.sh	2018-11-28 18:35:46.231956573 +0000
+++ s5d_limited/run-4-anydecode.sh	2018-12-20 17:17:48.765017308 +0000
@@ -1,4 +1,9 @@
 #!/bin/bash
+
+# Bryan Li (bl2557), Xinyue Wang (xw2368)
+# This script decodes the final LDA+MLLT (tri4) alignment.
+# Our contributions detailed in comments below.
+
 set -e
 set -o pipefail
 
@@ -6,16 +11,16 @@
 . ./lang.conf || exit 1;
 
 
-dir=dev10h.pem
+dir=dev2h.pem
 kind=
 data_only=false
-fast_path=true
+fast_path=false
 skip_kws=true
 skip_stt=false
 skip_scoring=
 extra_kws=true
 vocab_kws=false
-tri5_only=false
+tri4_only=true # do not decode after tri4
 wip=0.5
 
 nnet3_model=nnet3/tdnn_sp
@@ -148,14 +153,14 @@
 unset dir
 unset kind
 
-function make_plp {
+function make_mfcc {
   target=$1
   logdir=$2
   output=$3
   if $use_pitch; then
-    steps/make_plp_pitch.sh --cmd "$decode_cmd" --nj $my_nj $target $logdir $output
+    steps/make_mfcc_pitch.sh --cmd "$decode_cmd" --nj $my_nj $target $logdir $output
   else
-    steps/make_plp.sh --cmd "$decode_cmd" --nj $my_nj $target $logdir $output
+    steps/make_mfcc.sh --cmd "$decode_cmd" --nj $my_nj $target $logdir $output
   fi
   utils/fix_data_dir.sh $target
   steps/compute_cmvn_stats.sh $target $logdir $output
@@ -268,91 +273,91 @@
     exit 1
   fi
 
-  if [ ! -f ${dataset_dir}/.plp.done ]; then
+  if [ ! -f ${dataset_dir}/.mfcc.done ]; then
     echo ---------------------------------------------------------------------
     echo "Preparing ${dataset_kind} parametrization files in ${dataset_dir} on" `date`
     echo ---------------------------------------------------------------------
-    make_plp ${dataset_dir} exp/make_plp/${dataset_id} plp
-    touch ${dataset_dir}/.plp.done
+    make_mfcc ${dataset_dir} exp/make_mfcc/${dataset_id} mfcc
+    touch ${dataset_dir}/.mfcc.done
   fi
   touch $dataset_dir/.done
 fi
 
-if  [ ! -f ${dataset_dir}_hires/.mfcc.done ]; then
-  dataset=$(basename $dataset_dir)
-  echo ---------------------------------------------------------------------
-  echo "Preparing ${dataset_kind} MFCC features in  ${dataset_dir}_hires on "`date`
-  echo ---------------------------------------------------------------------
-  if [ ! -d ${dataset_dir}_hires ]; then
-    utils/copy_data_dir.sh data/$dataset data/${dataset}_hires
-  fi
+# if  [ ! -f ${dataset_dir}_hires/.mfcc.done ]; then
+  # dataset=$(basename $dataset_dir)
+  # echo ---------------------------------------------------------------------
+  # echo "Preparing ${dataset_kind} MFCC features in  ${dataset_dir}_hires on "`date`
+  # echo ---------------------------------------------------------------------
+  # if [ ! -d ${dataset_dir}_hires ]; then
+    # utils/copy_data_dir.sh data/$dataset data/${dataset}_hires
+  # fi
+
+  # mfccdir=mfcc_hires
+  # steps/make_mfcc_pitch_online.sh --nj $my_nj --mfcc-config conf/mfcc_hires.conf \
+      # --cmd "$train_cmd" ${dataset_dir}_hires exp/make_mfcc_hires/$dataset $mfccdir;
+  # steps/compute_cmvn_stats.sh data/${dataset}_hires exp/make_mfcc_hires/${dataset} $mfccdir;
+  # utils/fix_data_dir.sh ${dataset_dir}_hires;
+
+  # utils/data/limit_feature_dim.sh 0:39 \
+    # data/${dataset}_hires data/${dataset}_hires_nopitch || exit 1;
+  # steps/compute_cmvn_stats.sh \
+    # data/${dataset}_hires_nopitch exp/make_hires/${dataset}_nopitch $mfccdir || exit 1;
+  # utils/fix_data_dir.sh data/${dataset}_hires_nopitch
+  # touch ${dataset_dir}_hires/.mfcc.done
+
+  # touch ${dataset_dir}_hires/.done
+# fi
+
+# if [ -f exp/nnet3/extractor/final.ie ] && \
+  # [ ! -f exp/nnet3/ivectors_$(basename $dataset_dir)/.done ] ;  then
+  # dataset=$(basename $dataset_dir)
 
-  mfccdir=mfcc_hires
-  steps/make_mfcc_pitch_online.sh --nj $my_nj --mfcc-config conf/mfcc_hires.conf \
-      --cmd "$train_cmd" ${dataset_dir}_hires exp/make_mfcc_hires/$dataset $mfccdir;
-  steps/compute_cmvn_stats.sh data/${dataset}_hires exp/make_mfcc_hires/${dataset} $mfccdir;
-  utils/fix_data_dir.sh ${dataset_dir}_hires;
-
-  utils/data/limit_feature_dim.sh 0:39 \
-    data/${dataset}_hires data/${dataset}_hires_nopitch || exit 1;
-  steps/compute_cmvn_stats.sh \
-    data/${dataset}_hires_nopitch exp/make_hires/${dataset}_nopitch $mfccdir || exit 1;
-  utils/fix_data_dir.sh data/${dataset}_hires_nopitch
-  touch ${dataset_dir}_hires/.mfcc.done
-
-  touch ${dataset_dir}_hires/.done
-fi
-
-if [ -f exp/nnet3/extractor/final.ie ] && \
-  [ ! -f exp/nnet3/ivectors_$(basename $dataset_dir)/.done ] ;  then
-  dataset=$(basename $dataset_dir)
+  # steps/online/nnet2/extract_ivectors_online.sh --cmd "$train_cmd" --nj $my_nj \
+    # ${dataset_dir}_hires exp/nnet3/extractor exp/nnet3/ivectors_$dataset || exit 1;
 
-  steps/online/nnet2/extract_ivectors_online.sh --cmd "$train_cmd" --nj $my_nj \
-    ${dataset_dir}_hires exp/nnet3/extractor exp/nnet3/ivectors_$dataset || exit 1;
+  # touch exp/nnet3/ivectors_$dataset/.done
+# fi
 
-  touch exp/nnet3/ivectors_$dataset/.done
-fi
+####################################################################
 
-#####################################################################
-#
 # KWS data directory preparation
-#
-#####################################################################
-echo ---------------------------------------------------------------------
-echo "Preparing kws data files in ${dataset_dir} on" `date`
-echo ---------------------------------------------------------------------
-lang=data/lang
-if [ ! -f data/dev10h.pem/.done.kws.dev ] ; then
-  if ! $skip_kws  ; then
-    if  $extra_kws ; then
-      L1_lex=data/local/lexiconp.txt
-      . ./local/datasets/extra_kws.sh || exit 1
-    fi
-    if  $vocab_kws ; then
-      . ./local/datasets/vocab_kws.sh || exit 1
-    fi
-    if [ ! -f data/lang.phn/G.fst ] ; then
-      ./local/syllab/run_phones.sh --stage -2 ${dataset_dir}
-    else
-      ./local/syllab/run_phones.sh ${dataset_dir}
-    fi
 
-    if [ ! -f data/lang.syll/G.fst ] ; then
-      ./local/syllab/run_syllabs.sh --stage -2  ${dataset_dir}
-    else
-      ./local/syllab/run_syllabs.sh ${dataset_dir}
-    fi
-
-    ./local/search/run_search.sh --dir ${dataset_dir##*/}
-    ./local/search/run_phn_search.sh --dir ${dataset_dir##*/}
-    ./local/search/run_syll_search.sh --dir ${dataset_dir##*/}
-  fi
-fi
-
-if $data_only ; then
-  echo "Exiting, as data-only was requested..."
-  exit 0;
-fi
+####################################################################
+# echo ---------------------------------------------------------------------
+# echo "Preparing kws data files in ${dataset_dir} on" `date`
+# echo ---------------------------------------------------------------------
+# lang=data/lang
+# if [ ! -f data/dev10h.pem/.done.kws.dev ] ; then
+  # if ! $skip_kws  ; then
+    # if  $extra_kws ; then
+      # L1_lex=data/local/lexiconp.txt
+      # . ./local/datasets/extra_kws.sh || exit 1
+    # fi
+    # if  $vocab_kws ; then
+      # . ./local/datasets/vocab_kws.sh || exit 1
+    # fi
+    # if [ ! -f data/lang.phn/G.fst ] ; then
+      # ./local/syllab/run_phones.sh --stage -2 ${dataset_dir}
+    # else
+      # ./local/syllab/run_phones.sh ${dataset_dir}
+    # fi
+
+    # if [ ! -f data/lang.syll/G.fst ] ; then
+      # ./local/syllab/run_syllabs.sh --stage -2  ${dataset_dir}
+    # else
+      # ./local/syllab/run_syllabs.sh ${dataset_dir}
+    # fi
+
+    # ./local/search/run_search.sh --dir ${dataset_dir##*/}
+    # ./local/search/run_phn_search.sh --dir ${dataset_dir##*/}
+    # ./local/search/run_syll_search.sh --dir ${dataset_dir##*/}
+  # fi
+# fi
+
+# if $data_only ; then
+  # echo "Exiting, as data-only was requested..."
+  # exit 0;
+# fi
 
 ####################################################################
 ##
@@ -360,7 +365,7 @@
 ##
 ####################################################################
 if [ ! -f data/langp_test/.done ]; then
-  cp -R data/langp/tri5_ali/ data/langp_test
+  cp -R data/langp/tri4/ data/langp_test
   cp data/lang/G.fst data/langp_test
   touch data/langp_test/.done
 fi
@@ -372,40 +377,46 @@
   ln -s lang.phn data/langp_test.phn
 fi
 
-
-decode=exp/tri5/decode_${dataset_id}
+# tri4 is final, instead of tri5_ali
+decode=exp/tri4/decode_${dataset_id}
 if [ ! -f ${decode}/.done ]; then
   echo ---------------------------------------------------------------------
   echo "Spawning decoding with SAT models  on" `date`
   echo ---------------------------------------------------------------------
   utils/mkgraph.sh \
-    data/langp_test exp/tri5 exp/tri5/graph |tee exp/tri5/mkgraph.log
+    data/langp_test exp/tri4 exp/tri4/graph |tee exp/tri4/mkgraph.log
 
   mkdir -p $decode
   #By default, we do not care about the lattices for this step -- we just want the transforms
   #Therefore, we will reduce the beam sizes, to reduce the decoding times
   steps/decode_fmllr_extra.sh --skip-scoring true --beam 10 --lattice-beam 4\
     --nj $my_nj --cmd "$decode_cmd" "${decode_extra_opts[@]}"\
-    exp/tri5/graph ${dataset_dir} ${decode} |tee ${decode}/decode.log
+    exp/tri4/graph ${dataset_dir} ${decode} |tee ${decode}/decode.log
   touch ${decode}/.done
 fi
 
 if ! $fast_path ; then
-  local/run_kws_stt_task2.sh --cer $cer --max-states $max_states \
+  local/run_kws_stt_task2.sh --cer 1 --max-states $max_states \
     --skip-scoring $skip_scoring --extra-kws $extra_kws --wip $wip \
     --cmd "$decode_cmd" --skip-kws $skip_kws --skip-stt $skip_stt \
     "${lmwt_plp_extra_opts[@]}" \
     ${dataset_dir} data/langp_test ${decode}
 
-  local/run_kws_stt_task2.sh --cer $cer --max-states $max_states \
+  local/run_kws_stt_task2.sh --cer 1 --max-states $max_states \
     --skip-scoring $skip_scoring --extra-kws $extra_kws --wip $wip \
     --cmd "$decode_cmd" --skip-kws $skip_kws --skip-stt $skip_stt  \
     "${lmwt_plp_extra_opts[@]}" \
     ${dataset_dir} data/langp_test ${decode}.si
 fi
 
-if $tri5_only; then
-  echo "--tri5-only is true. So exiting."
+# print best CER and WER to console
+echo "Best CER"
+grep Sum ${decode}/score_*/*pem.char.ctm.sys | utils/best_wer.sh
+echo "Best WER"
+grep Sum ${decode}/score_*/*pem.ctm.sys | utils/best_wer.sh
+
+if $tri4_only; then
+  echo "--tri4-only is true. So exiting."
   exit 0
 fi
 
Only in s5d_limited: run_all.sh
